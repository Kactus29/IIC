{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46de76fc",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ef099c",
   "metadata": {},
   "source": [
    "#### PyTorch imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d217406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6329aa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.5.1\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch version', torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e143b94",
   "metadata": {},
   "source": [
    "#### General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17193829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458cb57",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b320248",
   "metadata": {},
   "source": [
    "#### Directories for the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ca50722",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent = Path().cwd().parent\n",
    "data_root = parent / 'try_1'\n",
    "\n",
    "train_images_dir = data_root / 'train' / 'images'\n",
    "train_masks_dir = data_root / 'train' / 'masks'\n",
    "\n",
    "val_images_dir = data_root / 'val' / 'images'\n",
    "val_masks_dir = data_root / 'val' / 'masks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0301f5b",
   "metadata": {},
   "source": [
    "#### General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca73aa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs: 100\n",
      "Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "IMG_HEIGHT = 768\n",
    "IMG_WIDTH = 768\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "EPOCHS = int(input('Enter number of epochs: '))\n",
    "print('Number of epochs:', EPOCHS)\n",
    "\n",
    "LR = float(input('Enter learning rate: '))\n",
    "print('Learning rate:', LR)\n",
    "\n",
    "\"\"\"\n",
    "0 : background (black)\n",
    "1 : arteries (white)\n",
    "2 : veins (gray)\n",
    "\"\"\"\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "COLOR_TO_LABEL = {\n",
    "    (0,0,0): 0,\n",
    "    (255,255,255): 1,\n",
    "    (128,128,128): 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2dce74",
   "metadata": {},
   "source": [
    "#### Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "798ae5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f15c217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(s) :\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed_all(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c3254d",
   "metadata": {},
   "source": [
    "# DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d4851",
   "metadata": {},
   "source": [
    "#### Create the class for the dataset images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d78b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAVIRDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, img_size=(IMG_HEIGHT, IMG_WIDTH), transforms=None):\n",
    "        self.img_paths = sorted(list(Path(img_dir).glob('*.png')))\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        self.img_size = img_size\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_p = self.img_paths[idx]\n",
    "        mask_p = self.mask_dir / img_p.name\n",
    "        img = Image.open(img_p).convert('RGB').resize(self.img_size, Image.BILINEAR)\n",
    "        img = np.array(img, dtype=np.float32) / 255.0\n",
    "        img = np.transpose(img, (2,0,1))  # C,H,W\n",
    "        img_t = torch.from_numpy(img).float()\n",
    "        # load mask and map colors to labels\n",
    "        m = Image.open(mask_p).convert('RGB').resize(self.img_size, Image.NEAREST)\n",
    "        m_arr = np.array(m, dtype=np.uint8)\n",
    "        label = np.zeros((self.img_size[1], self.img_size[0]), dtype=np.uint8)\n",
    "        for color, lab in COLOR_TO_LABEL.items():\n",
    "            mask = np.all(m_arr == np.array(color, dtype=np.uint8), axis=-1)\n",
    "            label[mask] = lab\n",
    "        label_t = torch.from_numpy(label).long()  # H,W\n",
    "        return img_t, label_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9fa41e",
   "metadata": {},
   "source": [
    "#### Actually create the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1989d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = RAVIRDataset(train_images_dir, train_masks_dir, (IMG_WIDTH, IMG_HEIGHT))\n",
    "val_ds = RAVIRDataset(val_images_dir, val_masks_dir, (IMG_WIDTH, IMG_HEIGHT))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbbfebb",
   "metadata": {},
   "source": [
    "# UNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a1e82",
   "metadata": {},
   "source": [
    "![Image de l'architecture UNet](unet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f228f7",
   "metadata": {},
   "source": [
    "#### Conv 3x3, ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bcb210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoubleConv(in_ch, out_ch) :\n",
    "    \"\"\"\n",
    "    Creates a bloc :\n",
    "        Conv 3x3 -> ReLU -> Conv 3x3 -> ReLU\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054f4c99",
   "metadata": {},
   "source": [
    "#### Max pool 2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88ffdb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_block(in_ch, out_ch) :\n",
    "    \"\"\"\n",
    "    Encoder bloc : \n",
    "        DoubleConv -> MaxPool 2x2\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        DoubleConv(in_ch, out_ch),\n",
    "        nn.MaxPool2d(kernel_size=2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c392f11",
   "metadata": {},
   "source": [
    "#### Exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be09ee8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768, 768])\n",
      "torch.Size([1, 64, 384, 384])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 768, 768)\n",
    "\n",
    "encode1 = down_block(1, 64)\n",
    "y = encode1(x)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d73af",
   "metadata": {},
   "source": [
    "#### Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08f50691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bottleneck(in_ch, out_ch) :\n",
    "    \"\"\"\n",
    "    Central part of UNet\n",
    "    \"\"\"\n",
    "    return DoubleConv(in_ch, out_ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c235901",
   "metadata": {},
   "source": [
    "#### UpSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8025d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_block(in_ch, out_ch) :\n",
    "    \"\"\"\n",
    "    Decoder bloc :\n",
    "        UpConv 2x2 -> Concatenate -> DoubleConv\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2),\n",
    "        DoubleConv(in_ch, out_ch)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "004db847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_up(block, x, skip_x):\n",
    "    \"\"\"\n",
    "    Forward step for the upsampling block in UNet.\n",
    "    - block: contains 'up' (ConvTranspose2d) and 'conv' (double_conv module)\n",
    "    - x: feature map coming from the previous decoder step (or bottleneck)\n",
    "    - skip_x: feature map from the corresponding encoder layer (skip connection)\n",
    "    \"\"\"\n",
    "    # 1) Upsample the decoder feature map (x) to double its spatial size\n",
    "    x = block[\"up\"](x)\n",
    "\n",
    "    # 2) Sometimes due to pooling/odd dimensions shapes might not perfectly match.\n",
    "    #    If so, we pad x to match skip_x's height and width.\n",
    "    if x.shape[-2:] != skip_x.shape[-2:]:\n",
    "        x = nn.functional.pad(\n",
    "            x,\n",
    "            [0, skip_x.shape[-1] - x.shape[-1],  # pad width\n",
    "             0, skip_x.shape[-2] - x.shape[-2]]  # pad height\n",
    "        )\n",
    "\n",
    "    # 3) Concatenate along the channel dimension: (batch, C_decoder + C_encoder, H, W)\n",
    "    x = torch.cat([skip_x, x], dim=1)\n",
    "\n",
    "    # 4) Apply the double convolution to fuse encoder and decoder features\n",
    "    x = block[\"conv\"](x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd13b0",
   "metadata": {},
   "source": [
    "#### Assembling of those blocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a739e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module) :\n",
    "    def __init__(self, in_channels=1, out_classes=1) :\n",
    "        \"\"\"\n",
    "        U-Net full architecture assembly.\n",
    "        - in_channels  : number of channels in input image (1=grayscale, 3=RGB)\n",
    "        - out_classes  : number of output channels (1=binary mask, N=multi-class)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder path\n",
    "        self.down1 = down_block(in_channels, 64)\n",
    "        self.down2 = down_block(64, 128)\n",
    "        self.down3 = down_block(128, 256)\n",
    "        self.down4 = down_block(256, 512)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = bottleneck(512, 1024)\n",
    "\n",
    "        # Decoder path\n",
    "        self.up4 = up_block(1024, 512)\n",
    "        self.up3 = up_block(512, 256)\n",
    "        self.up2 = up_block(256, 128)\n",
    "        self.up1 = up_block(128, 64)\n",
    "\n",
    "        # Final 1x1 convolution to map to output classes\n",
    "        self.final_conv = nn.Conv2d(64, out_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        # Encoder\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(x4)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d4 = forward_up(self.up4, b, x4)\n",
    "        d3 = forward_up(self.up3, d4, x3)\n",
    "        d2 = forward_up(self.up2, d3, x2)\n",
    "        d1 = forward_up(self.up1, d2, x1)\n",
    "\n",
    "        # Final output layer\n",
    "        out = self.final_conv(d1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6b2df9",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98cb7a26",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, out_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# exemple 3 classes : fond / veine / artère\u001b[39;00m\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m768\u001b[39m, \u001b[38;5;241m768\u001b[39m)  \u001b[38;5;66;03m# batch=1, grayscale\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# ✅ doit donner [1, 3, 768, 768]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/iic/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/iic/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[30], line 39\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbottleneck(x4)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Decoder with skip connections\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m d4 \u001b[38;5;241m=\u001b[39m \u001b[43mforward_up\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m d3 \u001b[38;5;241m=\u001b[39m forward_up(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup3, d4, x3)\n\u001b[1;32m     41\u001b[0m d2 \u001b[38;5;241m=\u001b[39m forward_up(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup2, d3, x2)\n",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m, in \u001b[0;36mforward_up\u001b[0;34m(block, x, skip_x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mForward step for the upsampling block in UNet.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m- block: contains 'up' (ConvTranspose2d) and 'conv' (double_conv module)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m- x: feature map coming from the previous decoder step (or bottleneck)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m- skip_x: feature map from the corresponding encoder layer (skip connection)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 1) Upsample the decoder feature map (x) to double its spatial size\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mup\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m(x)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 2) Sometimes due to pooling/odd dimensions shapes might not perfectly match.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#    If so, we pad x to match skip_x's height and width.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m!=\u001b[39m skip_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]:\n",
      "File \u001b[0;32m~/miniconda3/envs/iic/lib/python3.10/site-packages/torch/nn/modules/container.py:143\u001b[0m, in \u001b[0;36mSequential.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(OrderedDict(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems())[idx]))\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_item_by_idx\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/iic/lib/python3.10/site-packages/torch/nn/modules/container.py:132\u001b[0m, in \u001b[0;36mSequential._get_item_by_idx\u001b[0;34m(self, iterator, idx)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the idx-th item of the iterator.\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 132\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m-\u001b[39msize \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m<\u001b[39m size:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is out of range\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "model = UNet(in_channels=1, out_classes=3)  # exemple 3 classes : fond / veine / artère\n",
    "x = torch.randn(1, 1, 768, 768)  # batch=1, grayscale\n",
    "y = model(x)\n",
    "\n",
    "print(y.shape)  # ✅ doit donner [1, 3, 768, 768]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
