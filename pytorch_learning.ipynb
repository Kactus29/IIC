{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46de76fc",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ef099c",
   "metadata": {},
   "source": [
    "#### PyTorch imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d217406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6329aa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.5.1\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch version', torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e143b94",
   "metadata": {},
   "source": [
    "#### General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17193829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9b8de7",
   "metadata": {},
   "source": [
    "#### Monai imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3953395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.losses import DiceLoss, FocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458cb57",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b320248",
   "metadata": {},
   "source": [
    "#### Directories for the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ca50722",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent = Path().cwd().parent\n",
    "data_root = parent / 'try_1'\n",
    "\n",
    "train_images_dir = data_root / 'train' / 'images'\n",
    "train_masks_dir = data_root / 'train' / 'masks'\n",
    "\n",
    "val_images_dir = data_root / 'val' / 'images'\n",
    "val_masks_dir = data_root / 'val' / 'masks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0301f5b",
   "metadata": {},
   "source": [
    "#### General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca73aa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs: 100\n",
      "Learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "IMG_HEIGHT = 768\n",
    "IMG_WIDTH = 768\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "EPOCHS = 100\n",
    "print('Number of epochs:', EPOCHS)\n",
    "\n",
    "LR = 1e-4\n",
    "print('Learning rate:', LR)\n",
    "\n",
    "\"\"\"\n",
    "0 : background (black)\n",
    "1 : arteries (white)\n",
    "2 : veins (gray)\n",
    "\"\"\"\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "COLOR_TO_LABEL = {\n",
    "    (0,0,0): 0,\n",
    "    (255,255,255): 1,\n",
    "    (128,128,128): 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2dce74",
   "metadata": {},
   "source": [
    "#### Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "798ae5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f15c217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(s) :\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed_all(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c3254d",
   "metadata": {},
   "source": [
    "# DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d4851",
   "metadata": {},
   "source": [
    "#### Create the class for the dataset images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d78b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAVIRDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, img_size=(IMG_HEIGHT, IMG_WIDTH), transforms=None):\n",
    "        self.img_paths = sorted(list(Path(img_dir).glob('*.png')))\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        self.img_size = img_size\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_p = self.img_paths[idx]\n",
    "        mask_p = self.mask_dir / img_p.name\n",
    "        img = Image.open(img_p).convert('RGB').resize(self.img_size, Image.BILINEAR)\n",
    "        img = np.array(img, dtype=np.float32) / 255.0\n",
    "        img = np.transpose(img, (2,0,1))  # C,H,W\n",
    "        img_t = torch.from_numpy(img).float()\n",
    "        # load mask and map colors to labels\n",
    "        m = Image.open(mask_p).convert('RGB').resize(self.img_size, Image.NEAREST)\n",
    "        m_arr = np.array(m, dtype=np.uint8)\n",
    "        label = np.zeros((self.img_size[1], self.img_size[0]), dtype=np.uint8)\n",
    "        for color, lab in COLOR_TO_LABEL.items():\n",
    "            mask = np.all(m_arr == np.array(color, dtype=np.uint8), axis=-1)\n",
    "            label[mask] = lab\n",
    "        label_t = torch.from_numpy(label).long()  # H,W\n",
    "        return img_t, label_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9fa41e",
   "metadata": {},
   "source": [
    "#### Actually create the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1989d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = RAVIRDataset(train_images_dir, train_masks_dir, (IMG_WIDTH, IMG_HEIGHT))\n",
    "val_ds = RAVIRDataset(val_images_dir, val_masks_dir, (IMG_WIDTH, IMG_HEIGHT))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbbfebb",
   "metadata": {},
   "source": [
    "# UNet Model V1 (not working)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a1e82",
   "metadata": {},
   "source": [
    "![Image de l'architecture UNet](unet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f228f7",
   "metadata": {},
   "source": [
    "#### Conv 3x3, ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bcb210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoubleConv(in_ch, out_ch) :\n",
    "    \"\"\"\n",
    "    Creates a bloc :\n",
    "        Conv 3x3 -> ReLU -> Conv 3x3 -> ReLU\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054f4c99",
   "metadata": {},
   "source": [
    "#### Max pool 2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88ffdb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_block(in_ch, out_ch) :\n",
    "    \"\"\"\n",
    "    Encoder bloc : \n",
    "        DoubleConv -> MaxPool 2x2\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        DoubleConv(in_ch, out_ch),\n",
    "        nn.MaxPool2d(kernel_size=2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c392f11",
   "metadata": {},
   "source": [
    "#### Exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be09ee8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768, 768])\n",
      "torch.Size([1, 64, 384, 384])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 768, 768)\n",
    "\n",
    "encode1 = down_block(1, 64)\n",
    "y = encode1(x)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d73af",
   "metadata": {},
   "source": [
    "#### Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08f50691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bottleneck(in_ch, out_ch) :\n",
    "    \"\"\"\n",
    "    Central part of UNet\n",
    "    \"\"\"\n",
    "    return DoubleConv(in_ch, out_ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c235901",
   "metadata": {},
   "source": [
    "#### UpSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8025d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_block(in_ch, out_ch) :\n",
    "    \"\"\"\n",
    "    Decoder bloc :\n",
    "        UpConv 2x2 -> Concatenate -> DoubleConv\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2),\n",
    "        DoubleConv(in_ch, out_ch)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "004db847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_up(block, x, skip_x):\n",
    "    \"\"\"\n",
    "    Forward step for the upsampling block in UNet.\n",
    "    - block: contains 'up' (ConvTranspose2d) and 'conv' (double_conv module)\n",
    "    - x: feature map coming from the previous decoder step (or bottleneck)\n",
    "    - skip_x: feature map from the corresponding encoder layer (skip connection)\n",
    "    \"\"\"\n",
    "    # 1) Upsample the decoder feature map (x) to double its spatial size\n",
    "    x = block[\"up\"](x)\n",
    "\n",
    "    # 2) Sometimes due to pooling/odd dimensions shapes might not perfectly match.\n",
    "    #    If so, we pad x to match skip_x's height and width.\n",
    "    if x.shape[-2:] != skip_x.shape[-2:]:\n",
    "        x = nn.functional.pad(\n",
    "            x,\n",
    "            [0, skip_x.shape[-1] - x.shape[-1],  # pad width\n",
    "             0, skip_x.shape[-2] - x.shape[-2]]  # pad height\n",
    "        )\n",
    "\n",
    "    # 3) Concatenate along the channel dimension: (batch, C_decoder + C_encoder, H, W)\n",
    "    x = torch.cat([skip_x, x], dim=1)\n",
    "\n",
    "    # 4) Apply the double convolution to fuse encoder and decoder features\n",
    "    x = block[\"conv\"](x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd13b0",
   "metadata": {},
   "source": [
    "#### Assembling of those blocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a739e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module) :\n",
    "    def __init__(self, in_channels=1, out_classes=1) :\n",
    "        \"\"\"\n",
    "        U-Net full architecture assembly.\n",
    "        - in_channels  : number of channels in input image (1=grayscale, 3=RGB)\n",
    "        - out_classes  : number of output channels (1=binary mask, N=multi-class)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder path\n",
    "        self.down1 = down_block(in_channels, 64)\n",
    "        self.down2 = down_block(64, 128)\n",
    "        self.down3 = down_block(128, 256)\n",
    "        self.down4 = down_block(256, 512)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = bottleneck(512, 1024)\n",
    "\n",
    "        # Decoder path\n",
    "        self.up4 = up_block(1024, 512)\n",
    "        self.up3 = up_block(512, 256)\n",
    "        self.up2 = up_block(256, 128)\n",
    "        self.up1 = up_block(128, 64)\n",
    "\n",
    "        # Final 1x1 convolution to map to output classes\n",
    "        self.final_conv = nn.Conv2d(64, out_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        # Encoder\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(x4)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        d4 = forward_up(self.up4, b, x4)\n",
    "        d3 = forward_up(self.up3, d4, x3)\n",
    "        d2 = forward_up(self.up2, d3, x2)\n",
    "        d1 = forward_up(self.up1, d2, x1)\n",
    "\n",
    "        # Final output layer\n",
    "        out = self.final_conv(d1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6b2df9",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98cb7a26",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, out_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# exemple 3 classes : fond / veine / artère\u001b[39;00m\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m768\u001b[39m, \u001b[38;5;241m768\u001b[39m)  \u001b[38;5;66;03m# batch=1, grayscale\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# ✅ doit donner [1, 3, 768, 768]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/iic/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/iic/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[14], line 39\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbottleneck(x4)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Decoder with skip connections\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m d4 \u001b[38;5;241m=\u001b[39m \u001b[43mforward_up\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m d3 \u001b[38;5;241m=\u001b[39m forward_up(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup3, d4, x3)\n\u001b[1;32m     41\u001b[0m d2 \u001b[38;5;241m=\u001b[39m forward_up(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup2, d3, x2)\n",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m, in \u001b[0;36mforward_up\u001b[0;34m(block, x, skip_x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mForward step for the upsampling block in UNet.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m- block: contains 'up' (ConvTranspose2d) and 'conv' (double_conv module)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m- x: feature map coming from the previous decoder step (or bottleneck)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m- skip_x: feature map from the corresponding encoder layer (skip connection)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 1) Upsample the decoder feature map (x) to double its spatial size\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mup\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m(x)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 2) Sometimes due to pooling/odd dimensions shapes might not perfectly match.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#    If so, we pad x to match skip_x's height and width.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m!=\u001b[39m skip_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]:\n",
      "File \u001b[0;32m~/miniconda3/envs/iic/lib/python3.10/site-packages/torch/nn/modules/container.py:143\u001b[0m, in \u001b[0;36mSequential.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(OrderedDict(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems())[idx]))\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_item_by_idx\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/iic/lib/python3.10/site-packages/torch/nn/modules/container.py:132\u001b[0m, in \u001b[0;36mSequential._get_item_by_idx\u001b[0;34m(self, iterator, idx)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the idx-th item of the iterator.\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 132\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m-\u001b[39msize \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m<\u001b[39m size:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is out of range\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "model = UNet(in_channels=1, out_classes=3)  # exemple 3 classes : fond / veine / artère\n",
    "x = torch.randn(1, 1, 768, 768)  # batch=1, grayscale\n",
    "y = model(x)\n",
    "\n",
    "print(y.shape)  # ✅ doit donner [1, 3, 768, 768]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b160b6bb",
   "metadata": {},
   "source": [
    "# UNet Model V2 (working but not very optimised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6981a123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 🔹 1. Double convolution block\n",
    "# ============================================================\n",
    "\n",
    "def double_conv(in_ch, out_ch):\n",
    "    \"\"\"\n",
    "    Basic building block of UNet:\n",
    "    Two consecutive convolution layers with ReLU activations.\n",
    "    Keeps the spatial size (thanks to padding=1).\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# 🔹 2. Downsampling block (Encoder)\n",
    "# ============================================================\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder part: applies double conv + max pooling\n",
    "    Reduces spatial size by 2 and increases feature channels.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x_pooled = self.pool(x)\n",
    "        return x, x_pooled   # we return both: the feature map (for skip) and the pooled one\n",
    "\n",
    "# ============================================================\n",
    "# 🔹 3. Bottleneck (bridge between encoder & decoder)\n",
    "# ============================================================\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    The bottom part of UNet — no pooling, just double conv.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# ============================================================\n",
    "# 🔹 4. Upsampling block (Decoder)\n",
    "# ============================================================\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder part: \n",
    "    - Upsamples using ConvTranspose2d\n",
    "    - Concatenates with corresponding encoder feature map (skip connection)\n",
    "    - Applies double convolution\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "        self.conv = double_conv(out_ch * 2, out_ch)\n",
    "\n",
    "    def forward(self, x, skip_x):\n",
    "        # 1. Upsample decoder feature map\n",
    "        x = self.up(x)\n",
    "\n",
    "        # 2. Fix possible size mismatch (due to odd input dimensions)\n",
    "        if x.shape[-2:] != skip_x.shape[-2:]:\n",
    "            x = F.pad(\n",
    "                x,\n",
    "                [0, skip_x.shape[-1] - x.shape[-1],\n",
    "                 0, skip_x.shape[-2] - x.shape[-2]]\n",
    "            )\n",
    "\n",
    "        # 3. Concatenate encoder and decoder features\n",
    "        x = torch.cat([skip_x, x], dim=1)\n",
    "\n",
    "        # 4. Fuse features with double conv\n",
    "        return self.conv(x)\n",
    "\n",
    "# ============================================================\n",
    "# 🔹 5. Full UNet assembly\n",
    "# ============================================================\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Full UNet model combining encoder, bottleneck and decoder.\n",
    "\n",
    "    Input:  (B, in_channels, H, W)\n",
    "    Output: (B, out_classes, H, W)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, out_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---- Encoder (contracting path) ----\n",
    "        self.down1 = DownBlock(in_channels, 64)\n",
    "        self.down2 = DownBlock(64, 128)\n",
    "        self.down3 = DownBlock(128, 256)\n",
    "        self.down4 = DownBlock(256, 512)\n",
    "\n",
    "        # ---- Bottleneck ----\n",
    "        self.bottleneck = Bottleneck(512, 1024)\n",
    "\n",
    "        # ---- Decoder (expanding path) ----\n",
    "        self.up4 = UpBlock(1024, 512)\n",
    "        self.up3 = UpBlock(512, 256)\n",
    "        self.up2 = UpBlock(256, 128)\n",
    "        self.up1 = UpBlock(128, 64)\n",
    "\n",
    "        # ---- Final output layer ----\n",
    "        self.final_conv = nn.Conv2d(64, out_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder forward path (with skip connections saved)\n",
    "        x1, p1 = self.down1(x)   # 64 channels\n",
    "        x2, p2 = self.down2(p1)  # 128 channels\n",
    "        x3, p3 = self.down3(p2)  # 256 channels\n",
    "        x4, p4 = self.down4(p3)  # 512 channels\n",
    "\n",
    "        # Bottleneck\n",
    "        bn = self.bottleneck(p4)\n",
    "\n",
    "        # Decoder path with skip connections\n",
    "        d4 = self.up4(bn, x4)\n",
    "        d3 = self.up3(d4, x3)\n",
    "        d2 = self.up2(d3, x2)\n",
    "        d1 = self.up1(d2, x1)\n",
    "\n",
    "        # Final 1x1 convolution to get per-pixel class logits\n",
    "        out = self.final_conv(d1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157b196a",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76730e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([1, 1, 768, 768])\n",
      "Output: torch.Size([1, 3, 768, 768])\n"
     ]
    }
   ],
   "source": [
    "model = UNet(in_channels=1, out_classes=3)   # grayscale → 3 classes\n",
    "x = torch.randn(1, 1, 768, 768)              # batch=1\n",
    "y = model(x)\n",
    "\n",
    "print(\"Input:\", x.shape)\n",
    "print(\"Output:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aa92cf",
   "metadata": {},
   "source": [
    "# UNet Model (more  optimised for the computations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c53fcc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_classes=NUM_CLASSES, in_channels=3, features=[32,64,128,256]):\n",
    "        super().__init__()\n",
    "        self.encs = nn.ModuleList()\n",
    "        self.pools = nn.ModuleList()\n",
    "        for f in features:\n",
    "            self.encs.append(DoubleConv(in_channels, f))\n",
    "            self.pools.append(nn.MaxPool2d(2))\n",
    "            in_channels = f\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        # decoder\n",
    "        self.upconvs = nn.ModuleList()\n",
    "        self.decs = nn.ModuleList()\n",
    "        for f in reversed(features):\n",
    "            self.upconvs.append(nn.ConvTranspose2d(f*2, f, kernel_size=2, stride=2))\n",
    "            self.decs.append(DoubleConv(f*2, f))\n",
    "        self.final_conv = nn.Conv2d(features[0], n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for enc, pool in zip(self.encs, self.pools):\n",
    "            x = enc(x)\n",
    "            skips.append(x)\n",
    "            x = pool(x)\n",
    "        x = self.bottleneck(x)\n",
    "        for up, dec, skip in zip(self.upconvs, self.decs, reversed(skips)):\n",
    "            x = up(x)\n",
    "            # pad if needed\n",
    "            if x.shape != skip.shape:\n",
    "                x = nn.functional.interpolate(x, size=skip.shape[2:])\n",
    "            x = torch.cat([skip, x], dim=1)\n",
    "            x = dec(x)\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "model = UNet().to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81391c4f",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf04efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4068504b",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0a40fe",
   "metadata": {},
   "source": [
    "#### Cross-Entropy Loss (CE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1f9aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29390de6",
   "metadata": {},
   "source": [
    "#### Dice Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a65fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = DiceLoss(to_onehot_y=True, softmax=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91d5b95",
   "metadata": {},
   "source": [
    "#### Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c818b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = FocalLoss(to_onehot_y=True, gamma=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ec0ff7",
   "metadata": {},
   "source": [
    "# Model train v0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa98045",
   "metadata": {},
   "source": [
    "Algorithm : \n",
    "\n",
    "```python\n",
    "for epoch in range(N_EPOCHS):\n",
    "    → Put the model in train mode\n",
    "    → Boucle on each batch :\n",
    "        - Forward pass\n",
    "        - Loss calcul\n",
    "        - Backward\n",
    "        - Update\n",
    "    → Switch the model in eval mode\n",
    "    → Compute validation loss\n",
    "    → Save the best model\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e1256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS+1) :\n",
    "    \n",
    "    # =========================\n",
    "    #        TRAIN\n",
    "    # =========================\n",
    "\n",
    "    # Active le mode entraînement du modèle\n",
    "    model.train() \n",
    "\n",
    "    running_loss = 0.0  # Somme cumulée des pertes sur l'ensemble du dataset\n",
    "    correct = 0         # Compteur de pixels correctement classés\n",
    "    total = 0           # Nombre total de pixels (pour calculer l'accuracy)\n",
    "\n",
    "    for imgs, masks in train_loader : # Parcours chaque batch du jeu d'entraînement\n",
    "\n",
    "        # Envoi des images des des masques sur le GPU (si disponible)\n",
    "        imgs = imgs.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # Réinitialisation des gradients avant chaque batch\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # Prédictions du modèle\n",
    "        outputs = model(imgs) \n",
    "\n",
    "        loss = criterion(outputs, masks)  # Calcul de la perte entre les prédictions et les masques réels\n",
    "        loss.backward()                   # Rétropropagation du gradient (calcul des dérivées)\n",
    "        optimizer.step()                  # Mise à jour des poids du modèle selon le gradient\n",
    "\n",
    "        # Ajout de la perte pondérée par la taille du batch\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "        preds = outputs.argmax(dim=1)     # Extraction de la classe prédite la plus probable (pixel par pixel)\n",
    "        correct += (preds == masks).sum().item()  # Comptage des pixels bien prédits\n",
    "        total += masks.numel()                     # Nombre total de pixels dans le batch\n",
    "\n",
    "    train_loss = running_loss / len(train_ds)  # Moyenne de la perte sur tout le dataset d'entraînement\n",
    "    train_acc = correct / total                # Taux de pixels correctement prédits sur le dataset\n",
    "\n",
    "\n",
    "    # =========================\n",
    "    #       VALIDATION\n",
    "    # =========================\n",
    "\n",
    "    model.eval() # Passage en mode évaluation\n",
    "\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad() : # Désactive le calcul des gradients pour économiser mémoire et temps\n",
    "\n",
    "        for imgs, masks in val_loader : # Parcours du dataset de validation\n",
    "\n",
    "            # Envoi des images des des masques sur le GPU (si disponible)\n",
    "            imgs = imgs.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            # Prédictions sur le batch de validation\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            loss = criterion(outputs, masks)  # Calcul de la perte de validation\n",
    "            val_loss += loss.item() * imgs.size(0)  # Accumulation pondérée par la taille du batch\n",
    "\n",
    "            preds = outputs.argmax(dim=1)           # Classes prédites\n",
    "            correct += (preds == masks).sum().item()  # Comptage des bons pixels\n",
    "            total += masks.numel()                    # Nombre total de pixels\n",
    "\n",
    "        val_loss = val_loss / len(val_ds)  # Moyenne de la perte sur la validation\n",
    "        val_acc = correct / total          # Accuracy globale sur la validation\n",
    "    \n",
    "    # =========================\n",
    "    #           LOGS\n",
    "    # =========================\n",
    "\n",
    "    print('-'*30)\n",
    "    print(f\"Epoch {epoch}/{EPOCHS+1}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\" val_loss : {val_loss:.4f} |  Val Acc: {val_acc:.4f}\")\n",
    "    print('-'*30)\n",
    "    print()      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4068d",
   "metadata": {},
   "source": [
    "#### Save the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3950fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = parent / 'try_1'\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_path = save_dir / 'unet_pytorch.pth'\n",
    "torch.save(model.state_dict(), str(model_path))\n",
    "print('Model state_dict saved to', model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9646b2",
   "metadata": {},
   "source": [
    "# Model train v1 (save the model inside the training loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8726bcf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 9.65 GiB of which 39.81 MiB is free. Including non-PyTorch memory, this process has 7.86 GiB memory in use. Process 219160 has 1.74 GiB memory in use. Of the allocated memory 7.46 GiB is allocated by PyTorch, and 146.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(imgs) \n\u001b[1;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, masks)  \u001b[38;5;66;03m# Calcul de la perte entre les prédictions et les masques réels\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m                   \u001b[38;5;66;03m# Rétropropagation du gradient (calcul des dérivées)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()                  \u001b[38;5;66;03m# Mise à jour des poids du modèle selon le gradient\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Ajout de la perte pondérée par la taille du batch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/iic/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/iic/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/iic/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 9.65 GiB of which 39.81 MiB is free. Including non-PyTorch memory, this process has 7.86 GiB memory in use. Process 219160 has 1.74 GiB memory in use. Of the allocated memory 7.46 GiB is allocated by PyTorch, and 146.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "best_val_loss = float(\"inf\")  # Pour garder la meilleure performance\n",
    "model_name = \"try_1\"          # Nom du dossier du modèle\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS+1) :\n",
    "    \n",
    "    # =========================\n",
    "    #        TRAIN\n",
    "    # =========================\n",
    "\n",
    "    # Active le mode entraînement du modèle\n",
    "    model.train() \n",
    "\n",
    "    running_loss = 0.0  # Somme cumulée des pertes sur l'ensemble du dataset\n",
    "    correct = 0         # Compteur de pixels correctement classés\n",
    "    total = 0           # Nombre total de pixels (pour calculer l'accuracy)\n",
    "\n",
    "    for imgs, masks in train_loader : # Parcours chaque batch du jeu d'entraînement\n",
    "\n",
    "        # Envoi des images des des masques sur le GPU (si disponible)\n",
    "        imgs = imgs.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # Réinitialisation des gradients avant chaque batch\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # Prédictions du modèle\n",
    "        outputs = model(imgs) \n",
    "\n",
    "        loss = criterion(outputs, masks)  # Calcul de la perte entre les prédictions et les masques réels\n",
    "        loss.backward()                   # Rétropropagation du gradient (calcul des dérivées)\n",
    "        optimizer.step()                  # Mise à jour des poids du modèle selon le gradient\n",
    "\n",
    "        # Ajout de la perte pondérée par la taille du batch\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "        preds = outputs.argmax(dim=1)     # Extraction de la classe prédite la plus probable (pixel par pixel)\n",
    "        correct += (preds == masks).sum().item()  # Comptage des pixels bien prédits\n",
    "        total += masks.numel()                     # Nombre total de pixels dans le batch\n",
    "\n",
    "    train_loss = running_loss / len(train_ds)  # Moyenne de la perte sur tout le dataset d'entraînement\n",
    "    train_acc = correct / total                # Taux de pixels correctement prédits sur le dataset\n",
    "\n",
    "\n",
    "    # =========================\n",
    "    #       VALIDATION\n",
    "    # =========================\n",
    "\n",
    "    model.eval() # Passage en mode évaluation\n",
    "\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad() : # Désactive le calcul des gradients pour économiser mémoire et temps\n",
    "\n",
    "        for imgs, masks in val_loader : # Parcours du dataset de validation\n",
    "\n",
    "            # Envoi des images des des masques sur le GPU (si disponible)\n",
    "            imgs = imgs.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            # Prédictions sur le batch de validation\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            loss = criterion(outputs, masks)  # Calcul de la perte de validation\n",
    "            val_loss += loss.item() * imgs.size(0)  # Accumulation pondérée par la taille du batch\n",
    "\n",
    "            preds = outputs.argmax(dim=1)           # Classes prédites\n",
    "            correct += (preds == masks).sum().item()  # Comptage des bons pixels\n",
    "            total += masks.numel()                    # Nombre total de pixels\n",
    "\n",
    "        val_loss = val_loss / len(val_ds)  # Moyenne de la perte sur la validation\n",
    "        val_acc = correct / total          # Accuracy globale sur la validation\n",
    "    \n",
    "    # =========================\n",
    "    #           LOGS\n",
    "    # =========================\n",
    "\n",
    "    print('-'*30)\n",
    "    print(f\"Epoch {epoch}/{EPOCHS+1}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss  : {val_loss:.4f} |  Val Acc: {val_acc:.4f}\")   \n",
    "\n",
    "    # =========================\n",
    "    #     SAVE BEST MODEL\n",
    "    # =========================\n",
    "\n",
    "    # Si la perte de validation s'améliore, on sauvegarde le modèle\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss  # On garde cette valeur comme la meilleure\n",
    "        print(f\"✅ Nouveau meilleur modèle trouvé (val_loss={val_loss:.4f})\")\n",
    "\n",
    "        # 📁 Création du dossier Models/<model_name> si besoin\n",
    "        save_dir = Path(\"Models\") / model_name\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # 1️⃣ Sauvegarde uniquement des poids (méthode recommandée)\n",
    "        state_dict_path = save_dir / \"best_model_state_dict.pth\"\n",
    "        torch.save(model.state_dict(), str(state_dict_path))\n",
    "\n",
    "        # 2️⃣ Sauvegarde du modèle complet (architecture + poids)\n",
    "        full_model_path = save_dir / \"best_model_full.pth\"\n",
    "        torch.save(model, str(full_model_path))\n",
    "\n",
    "        print(f\"💾 Modèle sauvegardé dans {save_dir}\")\n",
    "        \n",
    "        print('-'*30)\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
